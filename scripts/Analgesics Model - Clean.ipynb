{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook for Demand Analysis of the Painkiller Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "import seaborn as sns\n",
    "from linearmodels.iv import compare\n",
    "from collections import OrderedDict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "from linearmodels.iv import IV2SLS, IVGMM\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import pandas2ri\n",
    "pandas2ri.activate()\n",
    "\n",
    "# Set up Linear Model\n",
    "lr = LinearRegression()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for returning the week of a month, needed for data loading\n",
    "def week_of_month(dt):\n",
    "    \"\"\" Returns the week of the month for the specified date.\n",
    "    \"\"\"\n",
    "\n",
    "    first_day = pd.to_datetime(dt.replace(day=1))\n",
    "\n",
    "    dom = dt.day\n",
    "    adjusted_dom = dom + first_day.weekday()\n",
    "\n",
    "    return int(ceil(adjusted_dom/7.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/17867029/anaconda3/lib/python3.6/site-packages/rpy2/robjects/pandas2ri.py:191: FutureWarning: from_items is deprecated. Please use DataFrame.from_dict(dict(items), ...) instead. DataFrame.from_dict(OrderedDict(items)) may be used to preserve the key order.\n",
      "  res = PandasDataFrame.from_items(items)\n"
     ]
    }
   ],
   "source": [
    "# Setting Filepath\n",
    "filepath = '../data/'\n",
    "\n",
    "# Reading in RDS file\n",
    "readRDS = robjects.r['readRDS']\n",
    "df = readRDS(filepath + 'analgesics.RDS')\n",
    "data = pandas2ri.ri2py(df)\n",
    "\n",
    "#Removing rows where OK column is 0\n",
    "data = data[data.ok==1].reset_index(drop=True)\n",
    "\n",
    "# Removing rows where quantities are below 0.01, these weeks have nonsensical pricing data\n",
    "data = data[(data.quant_weekly>0.01)]\n",
    "\n",
    "# Adding constant for regression\n",
    "data['const'] = 1\n",
    "\n",
    "# Converting UPC data type\n",
    "data.upc = data.upc.astype(int).astype(str)\n",
    "\n",
    "# Removing Excederin from the data, due to the fact that its Aspirin + Paracetamol, not suitable for Nested Logit\n",
    "data = data[data.upc!='31981000166']\n",
    "\n",
    "# Matching Unique UPCs to descriptions\n",
    "upc_map=[]\n",
    "# Getting descriptions and sizes\n",
    "for upc in data.upc.unique():\n",
    "    des = data.loc[data.upc==upc,'descrip'].iloc[0]\n",
    "    size = data.loc[data.upc==upc,'nsize'].iloc[0]\n",
    "    upc_map.append([upc,des, size])\n",
    "\n",
    "# Full product names from description and size\n",
    "products = [upc_map[i][1] + '_' + str(upc_map[i][2]) for i in range(0, len(upc_map))]\n",
    "\n",
    "# Assigning groups by active ingredients, generic vs branded and sizes\n",
    "for upc, desc, size in upc_map:\n",
    "    if ('TYLENOL' in desc) | ('NON-ASP' in desc):\n",
    "        data.loc[data.upc==upc,'Nest']=1\n",
    "    elif ('ADVIL' in desc) | ('IBUPROFEN' in desc):\n",
    "        data.loc[data.upc==upc,'Nest']=2\n",
    "    else:\n",
    "        data.loc[data.upc==upc,'Nest']=3\n",
    "    #generics    \n",
    "    if ('DOM' in desc):\n",
    "        data.loc[data.upc==upc,'Generic']=1\n",
    "    else:\n",
    "        data.loc[data.upc==upc,'Generic']=0\n",
    "    #sizes\n",
    "    if (size==24) or (size==30):\n",
    "        data.loc[data.upc==upc,'size_group']=1\n",
    "    elif (size==50) or (size==60):\n",
    "        data.loc[data.upc==upc,'size_group']=2\n",
    "    else:\n",
    "        data.loc[data.upc==upc,'size_group']=3\n",
    "\n",
    "# Compute market share numbers\n",
    "\n",
    "# Weekly sales by UPC, week and store \n",
    "data.loc[:, 'Product_sales'] = data.groupby(['upc', 'week', 'store']).quant_weekly.transform(sum)\n",
    "\n",
    "# Weekly Sales per store\n",
    "data.loc[:, 'Weekly_sales'] = data.groupby(['week', 'store']).quant_weekly.transform(sum)\n",
    "\n",
    "# Market share of a given product as defined by Sales / Mean Customer Count\n",
    "data.loc[:, 'Market_share'] = data.Product_sales / data.msize_meancustcount_weekly\n",
    "\n",
    "# Outside Share\n",
    "data.loc[:, 'Outside_share'] = (data.msize_meancustcount_weekly - data.Weekly_sales)/ data.msize_meancustcount_weekly\n",
    "\n",
    "# Calculating Log difference between market share and outside share for estimation\n",
    "data.loc[:, 'log_diff'] = np.log(data.Market_share) - np.log(data.Outside_share)\n",
    "\n",
    "# Compute group market share numbers -by ingredient\n",
    "data.loc[:, 'Ingroup_sales'] = data.groupby(['upc', 'week', 'store', 'Nest']).quant_weekly.transform(sum)\n",
    "data.loc[:, 'Group_sales'] = data.groupby(['week', 'store','Nest']).quant_weekly.transform(sum)\n",
    "data.loc[:, 'Group_share_ing'] = np.log(data.Ingroup_sales / data.Group_sales)\n",
    "\n",
    "# Compute group market share numbers - by generic vs branded\n",
    "data.loc[:, 'Ingroup_sales_Gen'] = data.groupby(['upc', 'week', 'store', 'Generic']).quant_weekly.transform(sum)\n",
    "data.loc[:, 'Group_sales_Gen'] = data.groupby(['week', 'store','Generic']).quant_weekly.transform(sum)\n",
    "data.loc[:, 'Group_share_Gen'] = np.log(data.Ingroup_sales_Gen / data.Group_sales_Gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting exogenous variables\n",
    "\n",
    "#Customer Demographics\n",
    "demos = data.iloc[:,129:172].copy()\n",
    "\n",
    "#Sales Data\n",
    "sales = data.loc[:, ['salesum_b_help', 'sale_b_weekly','salesum_c_help', 'sale_c_weekly', \n",
    "                     'salesum_g_help','sale_g_weekly', 'salesum_s_help', 'sale_s_weekly',\n",
    "                     'sale_all_weekly']]\n",
    "\n",
    "#Chosen Control variables\n",
    "use_cols = ['age9', 'income', 'ethnic', 'hsizeavg', 'nocar', 'hvalmean', \n",
    "            'retired', 'unemp', 'educ', 'sale_all_weekly']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and preparing exchange rate data for Instrumental Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load historical currency rates\n",
    "# USD vs CNY\n",
    "CNY = pd.read_csv(filepath + 'USD_CNY_Historical_Data.csv')\n",
    "\n",
    "# USD vs INR\n",
    "INR = pd.read_csv(filepath + 'USD_INR_Historical_Data.csv')\n",
    "\n",
    "# EUR vs USD\n",
    "EUR = pd.read_csv(filepath + 'EUR_USD_Historical_Data.csv')\n",
    "# Inverse for correct specification\n",
    "EUR.Price = 1/ EUR.Price\n",
    "\n",
    "# Extract datetime variables\n",
    "CNY.Date = pd.to_datetime(CNY.Date)\n",
    "INR.Date = pd.to_datetime(INR.Date)\n",
    "EUR.Date = pd.to_datetime(EUR.Date)\n",
    "CNY['Month'] = CNY.Date.dt.month\n",
    "CNY['Year'] = CNY.Date.dt.year\n",
    "CNY['week_in_mon'] = CNY.Date.apply(week_of_month)\n",
    "INR['Month'] = INR.Date.dt.month\n",
    "INR['Year'] = INR.Date.dt.year\n",
    "INR['week_in_mon'] = INR.Date.apply(week_of_month)\n",
    "EUR['Month'] = EUR.Date.dt.month\n",
    "EUR['Year'] = EUR.Date.dt.year\n",
    "EUR['week_in_mon'] = EUR.Date.apply(week_of_month)\n",
    "\n",
    "# Transforming the original data to fit the dates\n",
    "data.date = pd.to_datetime(data.date)\n",
    "data['Month'] = data.date.dt.month\n",
    "data['Year'] = data.date.dt.year\n",
    "\n",
    "#Merge FX rates in data set\n",
    "gb = CNY.groupby(['Year', 'Month', 'week_in_mon']).Price.median().reset_index()\n",
    "gb.columns = ['Year', 'Month', 'week_in_mon', 'CNY_Px']\n",
    "data = data.merge(gb, on=['Year', 'Month', 'week_in_mon'], how='left')\n",
    "gb = INR.groupby(['Year', 'Month', 'week_in_mon']).Price.median().reset_index()\n",
    "gb.columns = ['Year', 'Month', 'week_in_mon', 'INR_Px']\n",
    "data = data.merge(gb, on=['Year', 'Month', 'week_in_mon'], how='left')\n",
    "gb = EUR.groupby(['Year', 'Month', 'week_in_mon']).Price.median().reset_index()\n",
    "gb.columns = ['Year', 'Month', 'week_in_mon', 'EUR_Px']\n",
    "data = data.merge(gb, on=['Year', 'Month', 'week_in_mon'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting wholesaleprices to be used in the IV-Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.concat((pd.get_dummies(data.size_group, prefix='Size_'),pd.get_dummies(data.firm_id, prefix='Firm_'),\n",
    "                    pd.get_dummies(data.Nest, prefix='Ingred_')), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:         whprice_weekly   R-squared:                       0.001\n",
      "Model:                            OLS   Adj. R-squared:                  0.001\n",
      "Method:                 Least Squares   F-statistic:                     122.2\n",
      "Date:                Fri, 16 Nov 2018   Prob (F-statistic):           4.48e-79\n",
      "Time:                        01:43:11   Log-Likelihood:             7.4754e+05\n",
      "No. Observations:              344796   AIC:                        -1.495e+06\n",
      "Df Residuals:                  344792   BIC:                        -1.495e+06\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "CNY_Px        -0.0006   4.43e-05    -12.610      0.000      -0.001      -0.000\n",
      "INR_Px      6.737e-05   1.37e-05      4.915      0.000    4.05e-05    9.42e-05\n",
      "EUR_Px         0.0138      0.001     13.542      0.000       0.012       0.016\n",
      "const          0.0491      0.001     60.872      0.000       0.047       0.051\n",
      "==============================================================================\n",
      "Omnibus:                   106757.022   Durbin-Watson:                   0.710\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            26341.917\n",
      "Skew:                          -0.436   Prob(JB):                         0.00\n",
      "Kurtosis:                       1.964   Cond. No.                         850.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Make dummies to predict wholesaleprices\n",
    "dummies = pd.concat((pd.get_dummies(data.size_group, prefix='Size_'),pd.get_dummies(data.firm_id, prefix='Firm_'),\n",
    "                    pd.get_dummies(data.Nest, prefix='Ingred_')), axis=1)\n",
    "\n",
    "# Dataframe with exogenous variables for wholesaleprices\n",
    "data_iv = data.loc[:, ['CNY_Px', 'INR_Px', 'EUR_Px', 'whprice_weekly' ,'const']]\n",
    "data_iv.fillna(method='ffill', inplace=True)\n",
    "\n",
    "\n",
    "# Extract wholesaleprices \n",
    "target = data_iv.whprice_weekly\n",
    "\n",
    "# Drop Wholesaleprices from previous dataframe\n",
    "data_iv.drop('whprice_weekly', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Drop exchange rates\n",
    "#data_iv.drop(['CNY_Px', 'INR_Px', 'EUR_Px'],axis=1, inplace=True)\n",
    "\n",
    "# Predict Wholesaleprices from Exchange rates\n",
    "reg_whPx = sm.OLS(endog=target, exog=data_iv, missing='drop', hasconst=True)\n",
    "\n",
    "# Model results\n",
    "results = reg_whPx.fit()\n",
    "print(results.summary())\n",
    "\n",
    "# Store predicted values in dataframe\n",
    "data['pred_whprice'] = results.predict(data_iv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CNY_Px</th>\n",
       "      <th>INR_Px</th>\n",
       "      <th>EUR_Px</th>\n",
       "      <th>const</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.7339</td>\n",
       "      <td>17.999</td>\n",
       "      <td>0.733676</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.7339</td>\n",
       "      <td>17.999</td>\n",
       "      <td>0.733676</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7339</td>\n",
       "      <td>17.999</td>\n",
       "      <td>0.733676</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.7339</td>\n",
       "      <td>17.999</td>\n",
       "      <td>0.733676</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.7339</td>\n",
       "      <td>17.999</td>\n",
       "      <td>0.733676</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CNY_Px  INR_Px    EUR_Px  const\n",
       "0  4.7339  17.999  0.733676      1\n",
       "1  4.7339  17.999  0.733676      1\n",
       "2  4.7339  17.999  0.733676      1\n",
       "3  4.7339  17.999  0.733676      1\n",
       "4  4.7339  17.999  0.733676      1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_iv.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Logit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up variables to be used as instruments \n",
    "\n",
    "# Competitors sales\n",
    "data['Comp_sales_all'] = data.groupby(['week', 'store']).sale_all_weekly.transform('sum') - data.sale_all_weekly\n",
    "\n",
    "# Competitor group sales\n",
    "data['Comp_gpsls_all'] = data.groupby(['week', 'store', 'Nest']).sale_all_weekly.transform('sum') - data.sale_all_weekly\n",
    "\n",
    "# Competitor size \n",
    "data['Comp_gpsls_size_all'] = data.groupby(['week', 'store', 'size_group']).sale_all_weekly.transform('sum') - data.sale_all_weekly\n",
    "\n",
    "# Competitor generic \n",
    "data['Comp_gpsls_gen_all'] = data.groupby(['week', 'store', 'Generic']).sale_all_weekly.transform('sum') - data.sale_all_weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up categoricals for dummies\n",
    "data['firm_id'] = data['firm_id'].astype('category')\n",
    "data['size_group'] = data['size_group'].astype('category')\n",
    "data['Nest'] = data['Nest'].astype('category')\n",
    "\n",
    "#Multinomial logit with demographics\n",
    "ml_demo = IV2SLS(dependent=data['log_diff'],\n",
    "            exog=data.loc[:,use_cols + ['const']],\n",
    "            endog=data['price_weekly'],\n",
    "            instruments=data['pred_whprice']).fit(cov_type='clustered', clusters=data.store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Model Comparison         \n",
      "===================================\n",
      "                         ML + Demos\n",
      "-----------------------------------\n",
      "Dep. Variable              log_diff\n",
      "Estimator                   IV-2SLS\n",
      "No. Observations             344796\n",
      "Cov. Est.                 clustered\n",
      "R-squared                   -17.627\n",
      "Adj. R-squared              -17.627\n",
      "F-statistic                  1144.9\n",
      "P-value (F-stat)             0.0000\n",
      "==================      ===========\n",
      "age9                         2.1261\n",
      "                           (0.6807)\n",
      "income                      -0.2715\n",
      "                          (-0.7546)\n",
      "ethnic                      -0.7216\n",
      "                          (-3.0899)\n",
      "hsizeavg                    -0.0790\n",
      "                          (-0.2807)\n",
      "nocar                       -0.0572\n",
      "                          (-0.0949)\n",
      "hvalmean                     0.0013\n",
      "                           (0.7897)\n",
      "retired                     -0.9129\n",
      "                          (-0.9359)\n",
      "unemp                       -0.4181\n",
      "                          (-0.1599)\n",
      "educ                         0.0825\n",
      "                           (0.1142)\n",
      "sale_all_weekly              1.9594\n",
      "                           (6.0580)\n",
      "const                       -10.700\n",
      "                          (-2.8412)\n",
      "price_weekly                 88.550\n",
      "                           (4.0030)\n",
      "==================== ==============\n",
      "Instruments            pred_whprice\n",
      "-----------------------------------\n",
      "\n",
      "T-stats reported in parentheses\n"
     ]
    }
   ],
   "source": [
    "results = OrderedDict()\n",
    "#results['ML_No Exog'] = ml_noexo\n",
    "results['ML + Demos'] = ml_demo\n",
    "print(compare(results))\n",
    "alpha = -ml_demo.params['price_weekly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 16s, sys: 40.6 ms, total: 8min 16s\n",
      "Wall time: 8min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Calculate elasticities\n",
    "# Takes about 8min\n",
    "n_upcs = len(data.upc.unique())\n",
    "elastic_iv = np.zeros((n_upcs, n_upcs))\n",
    "week_gp = data.groupby(['week','upc'])['price_weekly', 'Market_share'].median().reset_index()\n",
    "\n",
    "for i in range(0, n_upcs):\n",
    "    upc1 = data.upc.unique()[i]\n",
    "    \n",
    "    for j in range(i, n_upcs):\n",
    "        \n",
    "        upc2 = data.upc.unique()[j]\n",
    "        tmp=[]\n",
    "        tmp2=[]\n",
    "        for w in range(0, len(data.week.unique())):\n",
    "            week = data.week.unique()[w]\n",
    "            try:\n",
    "                px1 = week_gp.loc[(week_gp.week==week) & (week_gp.upc == upc1),'price_weekly'].values[0]\n",
    "                ms1 = week_gp.loc[(week_gp.week==week) & (week_gp.upc == upc1),'Market_share'].values[0]        \n",
    "                px2 = week_gp.loc[(week_gp.week==week) & (week_gp.upc == upc2),'price_weekly'].values[0]\n",
    "                ms2 = week_gp.loc[(week_gp.week==week) & (week_gp.upc == upc2),'Market_share'].values[0]\n",
    "                if i==j:\n",
    "                    # Own Price Elastictiy\n",
    "                    tmp.append(-alpha*(1-ms1)*px1)\n",
    "                else:\n",
    "                    # Cross Price Elasticity\n",
    "                    tmp.append(alpha*px2*ms2)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        if i==j:\n",
    "            elastic_iv[i,j] = np.nanmedian(tmp)\n",
    "        else:\n",
    "            elastic_iv[i,j] = np.nanmedian(tmp)\n",
    "            elastic_iv[j,i] = np.nanmedian(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Elasticities matrix\n",
    "products = [upc_map[i][1] + '_' + str(upc_map[i][2]) for i in range(0, len(upc_map))]\n",
    "\n",
    "# Make Dataframe\n",
    "elastic_iv_df = pd.DataFrame(elastic_iv, columns=products, index=products)\n",
    "own_elastic_iv = list(zip(products, np.round(np.diag(elastic_iv_df),2)))\n",
    "\n",
    "# Extract Own Pric \n",
    "own_df = pd.DataFrame(own_elastic_iv, columns = ['Product', 'Own Px Elasticity'])\n",
    "own_df.set_index('Product').sort_values('Own Px Elasticity').to_csv(filepath+'ML_own')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Own Px Elasticity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DOM COATED ASPIRIN_100</td>\n",
       "      <td>-0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DOM X/S NON-ASPIRIN_100</td>\n",
       "      <td>-0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DOM E/S NON-ASP CPLT_50</td>\n",
       "      <td>-0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DOM IBUPROFEN_100</td>\n",
       "      <td>-0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DOM IBUPROFEN_50</td>\n",
       "      <td>-0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TYLENOL X/S CAPLET_24</td>\n",
       "      <td>-1.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TYLENOL X/S CAPLET_50</td>\n",
       "      <td>-1.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TYLENOL X/S TABLETS_100</td>\n",
       "      <td>-0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TYLENOL X/S GELCAPS_100</td>\n",
       "      <td>-0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TYLENOL X/S GELCAPS_24</td>\n",
       "      <td>-1.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TYLENOL X/S CAPLET_100</td>\n",
       "      <td>-0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TYLENOL X/S TABS_30</td>\n",
       "      <td>-1.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TYLENOL X/S TABLETS_60</td>\n",
       "      <td>-0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ADVIL_50</td>\n",
       "      <td>-1.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ADVIL_100</td>\n",
       "      <td>-0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ADVIL CAPLET_24</td>\n",
       "      <td>-1.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ADVIL CAPLET_50</td>\n",
       "      <td>-1.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ANACIN TABS_100</td>\n",
       "      <td>-0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BAYER ASPIRIN_100</td>\n",
       "      <td>-0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ADVIL_24</td>\n",
       "      <td>-1.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>TYLENOL X/S GELCAPS_50</td>\n",
       "      <td>-1.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Product  Own Px Elasticity\n",
       "0    DOM COATED ASPIRIN_100              -0.26\n",
       "1   DOM X/S NON-ASPIRIN_100              -0.47\n",
       "2   DOM E/S NON-ASP CPLT_50              -0.53\n",
       "3         DOM IBUPROFEN_100              -0.58\n",
       "4          DOM IBUPROFEN_50              -0.72\n",
       "5     TYLENOL X/S CAPLET_24              -1.60\n",
       "6     TYLENOL X/S CAPLET_50              -1.15\n",
       "7   TYLENOL X/S TABLETS_100              -0.66\n",
       "8   TYLENOL X/S GELCAPS_100              -0.87\n",
       "9    TYLENOL X/S GELCAPS_24              -1.62\n",
       "10   TYLENOL X/S CAPLET_100              -0.72\n",
       "11      TYLENOL X/S TABS_30              -1.41\n",
       "12   TYLENOL X/S TABLETS_60              -0.94\n",
       "13                 ADVIL_50              -1.14\n",
       "14                ADVIL_100              -0.87\n",
       "15          ADVIL CAPLET_24              -1.63\n",
       "16          ADVIL CAPLET_50              -1.14\n",
       "17          ANACIN TABS_100              -0.62\n",
       "18        BAYER ASPIRIN_100              -0.53\n",
       "19                 ADVIL_24              -1.63\n",
       "20   TYLENOL X/S GELCAPS_50              -1.17"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "own_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nested Logit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by branded/generic within ingredient groups\n",
    "# Compute group market share numbers\n",
    "data.loc[:, 'Ingroup_sales_L2'] = data.groupby(['upc', 'week', 'store', 'Generic', 'Nest']).quant_weekly.transform(sum)\n",
    "data.loc[:, 'Group_sales_L2'] = data.groupby(['week', 'store','Generic', 'Nest']).quant_weekly.transform(sum)\n",
    "data.loc[:, 'Group_share_gen_ing'] = np.log(data.Ingroup_sales_L2 / data.Group_sales_L2)\n",
    "data['Comp_gpsls_L2_all'] = data.groupby(['week', 'store', 'Generic', 'Nest']).sale_all_weekly.transform('sum') - data.sale_all_weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested logit on Ingredients\n",
    "iv_nest_ing = IV2SLS(dependent=data['log_diff'],\n",
    "            exog=data.loc[:,use_cols +['const']],\n",
    "            endog=data.loc[:,['price_weekly', 'Group_share_ing']],\n",
    "            instruments=data.loc[:,['pred_whprice','Comp_gpsls_all']]).fit(cov_type='clustered', clusters=data.store)\n",
    "\n",
    "# Nested logit on Generic v Branded\n",
    "iv_nest_gen = IV2SLS(dependent=data['log_diff'],\n",
    "            exog=data.loc[:,use_cols +['const']],\n",
    "            endog=data.loc[:,['price_weekly', 'Group_share_Gen']],\n",
    "            instruments=data.loc[:,['pred_whprice','Comp_gpsls_gen_all']]).fit(cov_type='clustered', clusters=data.store)\n",
    "\n",
    "\n",
    "#Nested logit on double nest L2: Generic, L1: Ingredient\n",
    "iv_nest_ing_gen = IV2SLS(dependent=data['log_diff'],\n",
    "            exog=data.loc[:,use_cols +['const']],\n",
    "            endog=data.loc[:,['price_weekly', 'Group_share_Gen', 'Group_share_gen_ing']],\n",
    "            instruments=data.loc[:,['pred_whprice','Comp_gpsls_gen_all', 'Comp_gpsls_L2_all']]).fit(cov_type='clustered', clusters=data.store)\n",
    "\n",
    "# Extracting coefficients of the Nested Logit Model on Ingredients\n",
    "alpha = - iv_nest_ing.params['price_weekly']\n",
    "sigma = iv_nest_ing.params['Group_share_ing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Model Comparison</caption>\n",
       "<tr>\n",
       "             <td></td>                   <th>ML </th>            <th>Nest Gen.</th>          <th>Nest Ing.</th>     <th>Nest L1:Gen, L2:Ing</th>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Dep. Variable</th>              <td>log_diff</td>          <td>log_diff</td>           <td>log_diff</td>           <td>log_diff</td>      \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Estimator</th>                   <td>IV-2SLS</td>           <td>IV-2SLS</td>            <td>IV-2SLS</td>            <td>IV-2SLS</td>      \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations</th>            <td>344796</td>            <td>344796</td>             <td>344796</td>             <td>344796</td>       \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Cov. Est.</th>                  <td>clustered</td>         <td>clustered</td>          <td>clustered</td>          <td>clustered</td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>R-squared</th>                   <td>0.3328</td>            <td>0.5950</td>             <td>0.6209</td>             <td>0.5364</td>       \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Adj. R-squared</th>              <td>0.3328</td>            <td>0.5950</td>             <td>0.6209</td>             <td>0.5364</td>       \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>F-statistic</th>                 <td>3276.8</td>            <td>4771.8</td>             <td>9833.7</td>             <td>5459.5</td>       \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>P-value (F-stat)</th>            <td>0.0000</td>            <td>0.0000</td>             <td>0.0000</td>             <td>0.0000</td>       \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>=====================</th>     <td>===========</td>       <td>===========</td>        <td>===========</td>        <td>===========</td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age9</th>                        <td>3.8289</td>            <td>4.0242</td>             <td>4.2789</td>             <td>4.0891</td>       \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                          <td>(2.1443)</td>          <td>(2.1662)</td>           <td>(2.2768)</td>           <td>(2.2003)</td>      \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>income</th>                      <td>-0.2045</td>           <td>-0.1335</td>            <td>-0.1791</td>            <td>-0.1539</td>      \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                          <td>(-1.0951)</td>         <td>(-0.6514)</td>          <td>(-0.8437)</td>          <td>(-0.7530)</td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ethnic</th>                      <td>-0.4600</td>           <td>-0.5150</td>            <td>-0.5894</td>            <td>-0.5287</td>      \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                          <td>(-2.9861)</td>         <td>(-3.1366)</td>          <td>(-3.3419)</td>          <td>(-3.1840)</td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hsizeavg</th>                    <td>-0.1448</td>           <td>-0.1719</td>            <td>-0.1902</td>            <td>-0.1731</td>      \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                          <td>(-0.8500)</td>         <td>(-0.9364)</td>          <td>(-1.0367)</td>          <td>(-0.9434)</td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>nocar</th>                       <td>-0.1981</td>           <td>-0.1610</td>            <td>-0.2146</td>            <td>-0.1848</td>      \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                          <td>(-0.6336)</td>         <td>(-0.4796)</td>          <td>(-0.6093)</td>          <td>(-0.5467)</td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hvalmean</th>                    <td>0.0006</td>            <td>0.0008</td>             <td>0.0007</td>             <td>0.0008</td>       \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                          <td>(0.6031)</td>          <td>(0.7470)</td>           <td>(0.6801)</td>           <td>(0.7611)</td>      \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>retired</th>                     <td>-0.1228</td>           <td>0.0457</td>             <td>-0.0561</td>            <td>0.0197</td>       \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                          <td>(-0.3126)</td>         <td>(0.1100)</td>           <td>(-0.1346)</td>          <td>(0.0471)</td>      \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>unemp</th>                       <td>-0.1411</td>           <td>0.2979</td>             <td>0.6348</td>             <td>0.3005</td>       \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                          <td>(-0.1009)</td>         <td>(0.1965)</td>           <td>(0.4021)</td>           <td>(0.1972)</td>      \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>educ</th>                        <td>0.1832</td>            <td>0.0512</td>             <td>0.1332</td>             <td>0.0728</td>       \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                          <td>(0.3746)</td>          <td>(0.0953)</td>           <td>(0.2325)</td>           <td>(0.1347)</td>      \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sale_all_weekly</th>             <td>0.4819</td>            <td>0.3744</td>             <td>0.3196</td>             <td>0.4007</td>       \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                          <td>(28.708)</td>          <td>(15.869)</td>           <td>(17.506)</td>           <td>(16.308)</td>      \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                       <td>-3.1217</td>           <td>-3.5606</td>            <td>-3.0288</td>            <td>-3.6577</td>      \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                          <td>(-1.6719)</td>         <td>(-1.7468)</td>          <td>(-1.4208)</td>          <td>(-1.7967)</td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>price_weekly</th>                <td>-11.483</td>           <td>-4.8581</td>            <td>-6.3717</td>            <td>-3.6426</td>      \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                          <td>(-38.957)</td>         <td>(-6.4456)</td>          <td>(-20.684)</td>          <td>(-4.7684)</td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Group_share_Gen</th>            <td>        </td>           <td>0.3440</td>            <td>        </td>            <td>0.1200</td>       \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                          <td>        </td>          <td>(7.8623)</td>           <td>        </td>           <td>(2.3570)</td>      \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Group_share_ing</th>            <td>        </td>          <td>        </td>            <td>0.3908</td>            <td>        </td>      \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                          <td>        </td>          <td>        </td>           <td>(19.252)</td>           <td>        </td>      \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Group_share_gen_ing</th>        <td>        </td>          <td>        </td>           <td>        </td>            <td>0.2404</td>       \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                          <td>        </td>          <td>        </td>           <td>        </td>           <td>(18.387)</td>      \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>=======================</th> <td>==============</td> <td>====================</td> <td>================</td> <td>====================</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Instruments</th>              <td>pred_whprice</td>      <td>pred_whprice</td>       <td>pred_whprice</td>       <td>pred_whprice</td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th></th>                               <td></td>         <td>Comp_gpsls_gen_all</td>   <td>Comp_gpsls_all</td>   <td>Comp_gpsls_gen_all</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th></th>                               <td></td>                  <td></td>                   <td></td>           <td>Comp_gpsls_L2_all</td> \n",
       "</tr>\n",
       "</table><br/><br/>T-stats reported in parentheses<br/>id: 0x7fa3b509b470"
      ],
      "text/plain": [
       "                                         Model Comparison                                        \n",
       "=================================================================================================\n",
       "                                   ML             Nest Gen.        Nest Ing.  Nest L1:Gen, L2:Ing\n",
       "-------------------------------------------------------------------------------------------------\n",
       "Dep. Variable                 log_diff             log_diff         log_diff             log_diff\n",
       "Estimator                      IV-2SLS              IV-2SLS          IV-2SLS              IV-2SLS\n",
       "No. Observations                344796               344796           344796               344796\n",
       "Cov. Est.                    clustered            clustered        clustered            clustered\n",
       "R-squared                       0.3328               0.5950           0.6209               0.5364\n",
       "Adj. R-squared                  0.3328               0.5950           0.6209               0.5364\n",
       "F-statistic                     3276.8               4771.8           9833.7               5459.5\n",
       "P-value (F-stat)                0.0000               0.0000           0.0000               0.0000\n",
       "=====================      ===========          ===========      ===========          ===========\n",
       "age9                            3.8289               4.0242           4.2789               4.0891\n",
       "                              (2.1443)             (2.1662)         (2.2768)             (2.2003)\n",
       "income                         -0.2045              -0.1335          -0.1791              -0.1539\n",
       "                             (-1.0951)            (-0.6514)        (-0.8437)            (-0.7530)\n",
       "ethnic                         -0.4600              -0.5150          -0.5894              -0.5287\n",
       "                             (-2.9861)            (-3.1366)        (-3.3419)            (-3.1840)\n",
       "hsizeavg                       -0.1448              -0.1719          -0.1902              -0.1731\n",
       "                             (-0.8500)            (-0.9364)        (-1.0367)            (-0.9434)\n",
       "nocar                          -0.1981              -0.1610          -0.2146              -0.1848\n",
       "                             (-0.6336)            (-0.4796)        (-0.6093)            (-0.5467)\n",
       "hvalmean                        0.0006               0.0008           0.0007               0.0008\n",
       "                              (0.6031)             (0.7470)         (0.6801)             (0.7611)\n",
       "retired                        -0.1228               0.0457          -0.0561               0.0197\n",
       "                             (-0.3126)             (0.1100)        (-0.1346)             (0.0471)\n",
       "unemp                          -0.1411               0.2979           0.6348               0.3005\n",
       "                             (-0.1009)             (0.1965)         (0.4021)             (0.1972)\n",
       "educ                            0.1832               0.0512           0.1332               0.0728\n",
       "                              (0.3746)             (0.0953)         (0.2325)             (0.1347)\n",
       "sale_all_weekly                 0.4819               0.3744           0.3196               0.4007\n",
       "                              (28.708)             (15.869)         (17.506)             (16.308)\n",
       "const                          -3.1217              -3.5606          -3.0288              -3.6577\n",
       "                             (-1.6719)            (-1.7468)        (-1.4208)            (-1.7967)\n",
       "price_weekly                   -11.483              -4.8581          -6.3717              -3.6426\n",
       "                             (-38.957)            (-6.4456)        (-20.684)            (-4.7684)\n",
       "Group_share_Gen                                      0.3440                                0.1200\n",
       "                                                   (7.8623)                              (2.3570)\n",
       "Group_share_ing                                                       0.3908                     \n",
       "                                                                    (19.252)                     \n",
       "Group_share_gen_ing                                                                        0.2404\n",
       "                                                                                         (18.387)\n",
       "======================= ============== ==================== ================ ====================\n",
       "Instruments               pred_whprice         pred_whprice     pred_whprice         pred_whprice\n",
       "                                         Comp_gpsls_gen_all   Comp_gpsls_all   Comp_gpsls_gen_all\n",
       "                                                                                Comp_gpsls_L2_all\n",
       "-------------------------------------------------------------------------------------------------\n",
       "\n",
       "T-stats reported in parentheses\n",
       "IVModelComparison, id: 0x7fa3b509b470"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results Table\n",
    "\n",
    "resultsn = OrderedDict()\n",
    "resultsn['ML '] = ml_demo\n",
    "resultsn['Nest Gen.'] = iv_nest_gen\n",
    "resultsn['Nest Ing.'] = iv_nest_ing\n",
    "resultsn['Nest L1:Gen, L2:Ing'] = iv_nest_ing_gen\n",
    "#resultsn['Nest FE'] = iv_nest_fixed\n",
    "cmp = compare(resultsn)\n",
    "cmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 45s, sys: 190 ms, total: 4min 45s\n",
      "Wall time: 4min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Compute elasticities for nested logit\n",
    "# Takes about 4 min\n",
    "\n",
    "upc_df = pd.DataFrame(upc_map, columns=['upc', 'Name', 'Size'])\n",
    "week_gp = data.groupby(['week','upc'])['price_weekly', 'Market_share', \n",
    "                                           'Group_share_ing'].median().reset_index()\n",
    "\n",
    "weeks = len(data.week.unique())\n",
    "\n",
    "nests = data.Nest.unique()\n",
    "\n",
    "for nest in nests:\n",
    "    nest_upcs = data[data.Nest==nest].upc.unique()\n",
    "    elastic_x = np.zeros((len(nest_upcs), len(nest_upcs)))\n",
    "    products=[]\n",
    "    for u in nest_upcs:\n",
    "        products.append(str(upc_df.loc[upc_df.upc==u,'Name'].iloc[0]) + ' ' +\\\n",
    "                        str(upc_df.loc[upc_df.upc==u,'Size'].iloc[0]))\n",
    "    for i in range(0, len(nest_upcs)):\n",
    "        upc1 = nest_upcs[i]\n",
    "        g1 = nest\n",
    "        for j in range(i, len(nest_upcs)):\n",
    "            #print(i,j)\n",
    "            upc2 = nest_upcs[j]\n",
    "            g2 = nest\n",
    "            tmp=[]\n",
    "            tmp2=[]\n",
    "            for w in range(0, weeks):\n",
    "                week = data.week.unique()[w]\n",
    "                try:\n",
    "                    px1 = week_gp.loc[(week_gp.week==week) & (week_gp.upc == upc1),'price_weekly'].values[0]\n",
    "                    ms1 = week_gp.loc[(week_gp.week==week) & (week_gp.upc == upc1),'Market_share'].values[0]\n",
    "                    gs1 = week_gp.loc[(week_gp.week==week) & (week_gp.upc == upc1),'Group_share_ing'].values[0]            \n",
    "                    px2 = week_gp.loc[(week_gp.week==week) & (week_gp.upc == upc2),'price_weekly'].values[0]\n",
    "                    ms2 = week_gp.loc[(week_gp.week==week) & (week_gp.upc == upc2),'Market_share'].values[0]\n",
    "                    gs2 = week_gp.loc[(week_gp.week==week) & (week_gp.upc == upc2),'Group_share_ing'].values[0]\n",
    "                    if i==j:\n",
    "                        tmp.append(-alpha*px1*(1-sigma*gs1-(1-sigma)*ms1)/(1-sigma))\n",
    "                    elif g1==g2:\n",
    "                        tmp.append(-alpha*px2*(sigma*gs1+(1-sigma)*ms2)/(1-sigma))\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            if i==j:\n",
    "                elastic_x[i,j] = np.nanmedian(tmp)\n",
    "            else:\n",
    "                elastic_x[i,j] = np.nanmedian(tmp)\n",
    "    \n",
    "    # Elasticities matrix\n",
    "    if nest==3:\n",
    "        elastic_Asp = pd.DataFrame(elastic_x, columns=products, index=products)\n",
    "    elif nest==1:\n",
    "        elastic_Ace = pd.DataFrame(elastic_x, columns=products, index=products)\n",
    "    else:\n",
    "        elastic_Ibu = pd.DataFrame(elastic_x, columns=products, index=products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write to file\n",
    "\n",
    "(np.round(elastic_Asp,2)).to_csv(filepath+'Aspirin Nest')\n",
    "(np.round(elastic_Ace,2)).to_csv(filepath+'Acet Nest')\n",
    "(np.round(elastic_Ibu,2)).to_csv(filepath+'Ibu Nest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End of Document"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
